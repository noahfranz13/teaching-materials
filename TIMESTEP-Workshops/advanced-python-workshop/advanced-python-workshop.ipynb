{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353842f0-954a-4758-b47d-eaa8af44daa9",
   "metadata": {},
   "source": [
    "# More Advanced Python Workshop\n",
    "\n",
    "### Problem Definition\n",
    "Neutron stars are ultra-compact astronomical objects that are left remaining after a Core Collapse Supernova occurs. When massive stars are in binaries, multiple neutron stars can be left behind in a binary system. These binary neutron stars spiral around each other and eventually merge. But, because they are so dense, the binary neutron stars emit gravitational waves that we are able to observe with detectors on Earth.\n",
    "\n",
    "Kilonova is the name for the optical light we observe following a binary neutron star merger. Because a binary neutron star merger is a rapid event, we can watch the optical emission following the merger evolve over human time scales. This is called a \"light curve\" and is a plot of the light we see, or \"flux\", as a function of time.\n",
    "\n",
    "In the directory where this jupyter notebook is I've provided two things that we will use throughout this tutorial:\n",
    "1. A comma-separated file of the light curve of the only known kilonova, called AT2017gfo. This file is called `AT2017gfo-lightcurve.csv`\n",
    "2. A directory of a grid models of kilonova light curves. These models are from [Kasen+17](https://arxiv.org/abs/1710.05463)\n",
    "\n",
    "In this notebook, we will reproduce the model fitting results of [Kasen+17](https://arxiv.org/abs/1710.05463) by searching for the best model in the model grid of the AT2017gfo light curve! This will tell us unique properties about the kilonova that are otherwise not measureable.\n",
    "\n",
    "### Learning Goals\n",
    "By the end of going through this jupyter notebook I am hoping that you have built the following skills:\n",
    "1. A better understanding of reading code documentation and applying the functions you find in it\n",
    "2. Improved plotting skills, take some time to make your matplotlib plots pretty!\n",
    "3. How to \"pair code\", or work with a partner to write code\n",
    "4. How to use Astropy Tables and Units (these are super super useful for astronomy!)  \n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e2427-a5ce-42af-ad4f-fd805de02750",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"numpy<2\" matplotlib astropy h5py lightcurve-fitting scipy extinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b435cc54-14ae-49c7-994d-f6cabdf76bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for operating system related commands\n",
    "import glob # for gathering lots of files matching a common path\n",
    "\n",
    "# numpy and matplotlib, cause duh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# some astropy things\n",
    "from astropy.table import Table\n",
    "from astropy.io.misc.pyarrow.csv import read_csv\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from astropy import units as u\n",
    "\n",
    "# some things for parsing the model files\n",
    "import h5py\n",
    "from lightcurve_fitting.filters import filtdict\n",
    "from lightcurve_fitting.lightcurve import LC\n",
    "from scipy.signal import resample\n",
    "import extinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3459f4d5-fc2e-413b-9396-e4eaccddcbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import astropy\n",
    "astropy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce5629-0cca-4c9d-8f3c-2d7dd060dbd3",
   "metadata": {},
   "source": [
    "### Part 1: Reading in the AT2017gfo light curve\n",
    "\n",
    "Here is the documentation for reading in a CSV file into an astropy Table: https://docs.astropy.org/en/stable/api/astropy.io.misc.pyarrow.csv.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c67306-3a91-46d0-8430-57ede070f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "at2017gfo_table = # YOUR CODE HERE\n",
    "\n",
    "print(at2017gfo_table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845612bd-2e53-4c17-9fb6-80b60e5bc7f6",
   "metadata": {},
   "source": [
    "### Part 2: Plotting the light curve\n",
    "\n",
    "You should now have the AT2017gfo Table loaded into a variable called `at2017gfo_table`. Here is a description of the relevant column names (you can safely ignore the others):\n",
    "* `MJD` --> The Modified Julian Date, or MJD for short\n",
    "* `Phase` --> The time since discovery of the transient event\n",
    "* `Instrument` --> The instrument used for measuing this flux value\n",
    "* `Telescope` --> The telescope used for measuring this flux value\n",
    "* `Band` --> The filter used on the telescope for measuring this flux. We will focus on the \"r-band\", or the \"red\" filter, for this notebook.\n",
    "* `mag` --> The magnitude value for this flux. If you do not know what this is you should take some time reading [this wikipedia page](https://en.wikipedia.org/wiki/Magnitude_(astronomy)) and/or asking one of the grad coorinators.\n",
    "* `e_mag` --> The error on the magnitude.\n",
    "* `Ref` --> The reference for this flux value\n",
    "\n",
    "To access a column in the astropy table you do the following:\n",
    "```\n",
    "<table-name>[\"<column-name>\"]\n",
    "```\n",
    "So, if we want to access the `MJD` column we would do\n",
    "```\n",
    "at2017gfo[\"MJD\"]\n",
    "```\n",
    "\n",
    "In the following cell, make sure you understand by accessing the `mag` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe4454-9d54-4fcd-812d-dd005689ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_column = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cc9b3-d9c4-42a7-824c-c765ee2bb706",
   "metadata": {},
   "source": [
    "Now that we know how to access columns, let's briefly discuss boolean indexing. This allows you to pass boolean arrays as indices to the table to access only rows that satisfy those conditions. \n",
    "\n",
    "As an example, let's say we want to access all of the data that was taken with the \"Magellan\" telescope, we would do the following:\n",
    "```\n",
    "magellan_data = table[table[\"telescope\"] == \"Magellan\"]\n",
    "```\n",
    "\n",
    "We will now do something similar to access all of the rows that have the `Band == r`. Please do this in the following cell and save it in a variable called `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c933bd8-30f1-4741-a64d-6826b542c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721842a2-2c67-47ab-aca6-129636c3d645",
   "metadata": {},
   "source": [
    "Finally, now that we only have the r-band data for AT2017gfo, we want to plot a light curve for this event. As a reminder, that will be the `mag` column as a function of the `Phase` column. Please create this plot in the next cell:\n",
    "\n",
    "_Hint 1_: Remember that magnitudes are \"backwards\". Or, in other words, a smaller magnitude means brighter. What should we do to the y-axis to make this plot more intuitive?)\n",
    "\n",
    "_Hint 2_: Remember that there is a column with errors on the magnitude `e_mag`. You should plot these as errorbars. Checkout the docs for matplotlib `errorbar`.\n",
    "\n",
    "_Bonus_: Plot the light curve colored by the telescope name with a nice legend :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0bea0-37dc-45b6-baea-6113c13dd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Days Since Discovery\")\n",
    "ax.set_ylabel(\"Magnitude\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6414ba-222d-4b34-9a92-4329d788ea83",
   "metadata": {},
   "source": [
    "##############################################################################################################\n",
    "# PAUSE, CHECKPOINT 1\n",
    "Please show your light curve plot from the previous part to at least one graduate coordinator. You should also send this in slack for everyone to see your hard work!\n",
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc39ad2-1d80-4da0-b475-8bc25bddf64e",
   "metadata": {},
   "source": [
    "### Part 3: Reading and plotting the light curve models\n",
    "\n",
    "All of the models in the Kasen+17 simulation grid are stored in files called [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). While understanding how to work with these files is an important skill, learning it is out of the scope of this notebook. Furthermore, the Kasen+17 models are stored in three dimensions: 1) frequency; 2) time; 3) flux. This can make it difficult to understand how to parse them into a useable light curve (It took me an entire afternoon when I was originally doing this!). As a result, I've provided a python convenience function below for reading and parsing _a single model file_.\n",
    "\n",
    "Reading code is an important skill so you should take a few minutes and read this function to understand how it works. If you have any questions on how this function works, please ask a grad coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cb6d6-1f0f-4e7c-abe0-dd3359a3ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kasen17_model(model_file:str, filtname=\"r\", redshift=0.00984, ebv=0.109) -> Table:\n",
    "    \"\"\"\n",
    "    Takes in a path to a model file as a string and returns an astropy table with the following columns:\n",
    "    - filter --> The filter name\n",
    "    - mag --> The magnitude corresponding to this flux\n",
    "    - dt --> The days since discovery, usually called \"phase\"\n",
    "\n",
    "    Example:\n",
    "    To read in a file from Kasen+17 use\n",
    "    ```\n",
    "    lc = read_kasen17_model(\"kasen_kilonova_models_2017/knova_d1_n10_m0.020_vk0.10_Xlan1e-3.0.h5\")\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        model_file (str): The path to the Kasen+17 model file\n",
    "        filtname (str): The filter name to grab the data for. This is important for \n",
    "                        applying the proper transmission curve.\n",
    "        redshift (float): The redshift to the transient that we will fit these models to.\n",
    "                          Default is 0.00984 for AT2017gfo (from \n",
    "                          https://www.wis-tns.org/object/2017gfo)\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        An astropy Table with the data stored in the model\n",
    "    \"\"\"\n",
    "\n",
    "    # read in the file\n",
    "    fin = h5py.File(model_file,'r')\n",
    "\n",
    "    # parse various columns in the file, including the frequency \n",
    "    # (but convert to THz, hence the conversion using 1e-12)\n",
    "    nu = np.array(fin['nu'],dtype='d')*1e-12\n",
    "\n",
    "    # and the time\n",
    "    times = np.array(fin['time'])\n",
    "    times = times/3600.0/24.0\n",
    "\n",
    "    # and the specific luminosity \n",
    "    Lnu_all = np.array(fin['Lnu'],dtype='d')\n",
    "\n",
    "    # now generate a boolean array that only takes the data in the frequency range corresponding to `filtname`\n",
    "    freq_range = (\n",
    "        (filtdict[\"r\"].freq_eff - filtdict[\"r\"].dfreq).value, \n",
    "        (filtdict[\"r\"].freq_eff + filtdict[\"r\"].dfreq).value\n",
    "    )\n",
    "    where_filt = np.where(\n",
    "        (nu > min(freq_range)) * \n",
    "        (nu < max(freq_range))\n",
    "    )[0]\n",
    "\n",
    "    # we now need the transmission curve for this filter\n",
    "    # but we need to \"downsample\" it to the length of the times array, \n",
    "    # we do this using scipy.signal.resample\n",
    "    trans_curve = filtdict[filtname].trans\n",
    "    trans_resampled_idx = np.array([\n",
    "        np.argmin(\n",
    "            np.abs(\n",
    "                t-trans_curve[\"freq\"]\n",
    "            )\n",
    "        ) for t in nu\n",
    "    ])\n",
    "    trans_resampled = trans_curve[\"T\"][trans_resampled_idx]\n",
    "    \n",
    "    # finally, we integrate (\"sum\") along the frequency axis in the luminosity array\n",
    "    # while simultaneously applying the transmission function for this filter\n",
    "    Lnu = np.sum(\n",
    "        (Lnu_all * trans_resampled)[:,where_filt], \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # this gave us the light curve in luminosity space, but our data is in magnitude!\n",
    "    # so, we ust convert using the luminosity distance\n",
    "    # but, that means we need to find the lumdist from the redshift passed in,\n",
    "    # this is where the astropy cosmology package is super useful!\n",
    "    lumdist = cosmo.luminosity_distance(redshift)\n",
    "\n",
    "    # now we can convert to magnitudes using the normal equations\n",
    "    # (That are already programmed for you in this lightcurve_fitting package)\n",
    "    Fnu = (Lnu*u.erg/u.s/u.Hz /(4*np.pi*lumdist**2)).to(u.erg/u.s/u.cm**2/u.Hz)\n",
    "    zp = filtdict[\"r\"].fnu*u.W/u.m**2/u.Hz\n",
    "    lc = Table({\n",
    "        \"dt\":times,\n",
    "        \"mag\":-2.5*np.log10(Fnu/zp),\n",
    "        \"filter\":[filtname]*len(times)\n",
    "    })\n",
    "\n",
    "    # convert backwards from a LC object to it's superclass \"Table\"\n",
    "\n",
    "    # \"redden\" the models to simulate MW extinction\n",
    "    R_V = 3.1\n",
    "    A_V = R_V * ebv\n",
    "\n",
    "    filt_wave_eff = (filtdict[filtname].freq_eff).to(u.AA, equivalencies=u.spectral()).value\n",
    "    dust_law_mw = extinction.ccm89(np.array([filt_wave_eff]), A_V, R_V)\n",
    "    lc[\"_unextincted_mag\"] = lc[\"mag\"]\n",
    "    lc[\"mag\"] = lc[\"mag\"] + dust_law_mw[0]\n",
    "    \n",
    "    return lc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74dce3d-1c25-4ef5-a191-46e21497e505",
   "metadata": {},
   "source": [
    "Now, use this function to read in _at least one_ file in the `kasen_kilonova_models_2017` directory. \n",
    "\n",
    "_Hint 1_: Use the python `help(funcname)` function to read the docstring for this function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf56580-16bd-434b-8ca2-2bd6a8e41a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TODO: YOUR CODE HERE\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623b704-f3fa-4169-aec3-f1f796c096e7",
   "metadata": {},
   "source": [
    "### Part 4: Plot a Model\n",
    "\n",
    "Now that you've read in a model, plot the light curve for it! As a reminder, this will be `mag` vs. `dt`. Since this is simulated data (and I made up the `dmag` column to get the conversion to work with `lightcurve_fitting`), we don't need to plot any errorbars on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15188bda-a569-4cce-99d1-17e57f4b32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Days Since Discovery\")\n",
    "ax.set_ylabel(\"Magnitude\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119a3e1-3240-46f0-856f-87a0192e06e5",
   "metadata": {},
   "source": [
    "##############################################################################################################\n",
    "# PAUSE, CHECKPOINT 2\n",
    "Please show your light curve plot from the previous part to at least one graduate coordinator. You should also send this in slack for everyone to see your hard work!\n",
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8bc6f-ed9c-48b2-84cb-eb005b62296b",
   "metadata": {},
   "source": [
    "### Part 5: Sum of the Squares Residual\n",
    "\n",
    "Now that we know how to read in both the Kasen+17 model files and the AT2017gfo light curve, we need to write some code to find the best fitting model, automatically, The simplest way to do this is to use a least squares algorithm: https://en.wikipedia.org/wiki/Least_squares.\n",
    "\n",
    "If you go to that wikipedia link, you will see the following equation:\n",
    "$$\n",
    "S = \\Sigma_i r^2_i\n",
    "$$\n",
    "where $r = data-model$ is the residual and $S$ is the sum of the least square residuals.\n",
    "\n",
    "Please complete the following `sum_square_residual` function that I've begun for you:\n",
    "\n",
    "_Hint 1_: Assume that both of the inputs are numpy arrays. This means that you **should not** need to write any loops!\n",
    "\n",
    "_Hint 2_: Check out the numpy docs to see if they have anything for sums\n",
    "\n",
    "_Bonus_: Write a docstring for this function (see the example of a numpy formatted docstring above in `read_kasen17_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573f16a-b489-4897-9024-4bb7f07c3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_square_residual(data:np.ndarray, model:np.ndarray) -> np.ndarray:\n",
    "    residual = # TODO\n",
    "    residual_squared = # TODO\n",
    "    sum_residual_squared = # TODO\n",
    "    return sum_residual_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa818820-23f8-443e-92b5-b88fc28801f4",
   "metadata": {},
   "source": [
    "Since we need this function working for later parts, please run the following test code to make sure that it does work as expected! These are tests, `assert` means make sure the following statment is True, otherwise throw an AssertionError.\n",
    "\n",
    "If it throws any errors then you need to return to the above cell and fix some stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03cad0f-dbe5-4461-94e7-a0b6976869dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make sure this cell does not throw any errors when run\n",
    "assert sum_square_residual(np.array([1,2,3]), np.array([4,5,6])) == 27\n",
    "assert sum_square_residual(np.array([4,5,6]), np.array([1,2,3])) == 27\n",
    "assert sum_square_residual(np.array([1,2,3]), np.array([1,2,3])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01334a30-5322-42a3-8b1e-75834fd98700",
   "metadata": {},
   "source": [
    "### Part 6: Finding the best Kasen+17 model\n",
    "\n",
    "Now that we have a function to compute the sum of the square residual, we need to apply this to all of the models in `kasen_kilonova_models_2017` and find the model with the _least_ square residual. Please do this below:\n",
    "\n",
    "_REMINDER_: This is supposed to be hard! Give it a try, since we believe that coming up with algorithms like this is important. Butm we will reveal some more hints over time to help get you all started. \n",
    "\n",
    "_Hint 1_: You can use the built-in `glob` library (best library name in python IMO!) to get a list of all of the file names in the `kasen_kilonova_models_2017` directory\n",
    "```\n",
    "all_model_files = glob.glob(\"kasen_kilonova_models_2017/*h5\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cea0f5-ba9b-4d98-8bd9-f5beaf8666dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the model in the kasen_kilonova_models_2017 that has the lowest least squares residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365600b-9bb4-4b43-b6bb-b5c7e226330e",
   "metadata": {},
   "source": [
    "### Part 7: Plot the best fitting model over top the light curve\n",
    "\n",
    "This should basically just be a combination of your other plotting code from above. Take some time to make it pretty and then send it in slack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776effb8-a0ad-460a-8d14-af97afccd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel(\"Days Since Discovery\")\n",
    "ax.set_ylabel(\"Magnitude\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# TODO: Plot the light curve data\n",
    "# TODO: Plot the model on the same axis overtop of the light curve data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bd52f-a77e-4f5e-9bcd-6bc891b1fe59",
   "metadata": {},
   "source": [
    "##############################################################################################################\n",
    "# PAUSE, CHECKPOINT 3\n",
    "Please show your light curve plot from the previous part to at least one graduate coordinator. You should also send this in slack for everyone to see your hard work!\n",
    "###############################################################################################################\n",
    "\n",
    "### Extra Challenge\n",
    "\n",
    "If you've made it this far, congratulations! You are clearly a wizard at programming :) Here's a challenge for you to ponder and attempt:\n",
    "\n",
    "We are currently only finding the best fit model to the r-band light curve. However, there are a bunch of other filters in the `AT2017gfo-lightcurve.csv` file. In reality, we should be jointly fitting the light curve in three dimensional space (time, frequency, and flux/magnitude), using _all_ of the data in that file to find the best fit model across the entire spectrum. \n",
    "\n",
    "This is quite difficult, but I'd like you to try to do this! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aecf79-047a-4922-91f5-51fcf16de3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
